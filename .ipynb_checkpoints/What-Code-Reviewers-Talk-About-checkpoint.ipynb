{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Code Reviewers Talk About - A Machine Learning Experiment\n",
    "\n",
    "A code review is a form of code inspection where a developer assesses code for style, defects, and other standards prior to integration into a code base. As part of the code review process on GitHub, developers may leave comments on portions of the unified diff of a GitHub pull request. These comments are extremely valuable in factilitating technical discussion amongst developers, and in allowing developers to get feedback on their code submissions. In an effort to better understand code reivewing habbits, weâ€™re going to create an SVM classifier to classify over 30 000 GitHub review comments based on the main topic addressed by each comment (e.g. naming, readability, etc.).\n",
    "\n",
    "![review](https://zir0-93.github.io/images/i-was-told-there-would-be-a-review.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Comment Classifications\n",
    "\n",
    "The list of categories we're going to incorporate into our classifier are summarized in the table below. This list was developed based on a manual survey of \n",
    "approximately 2000 GitHub review comments I performed on randomly selected, but highly forked Java repositories on GitHub. The selected \n",
    "categories reflect the most frequently occurring topics encountered in the surveyed review comments. Majority of the categories \n",
    "are related to code level concepts (e.g. variable naming, exception handling); however, certain review comments \n",
    "that did not naturally fall into any existing categories and were unrelated to the overall goal of code reviewing were placed in\n",
    "the \"other\" category. In situations where a review comment discussed more than one subject, I gave it a classification according \n",
    "to the topic it spent the most words discussing.\n",
    "\n",
    "| Category| Label | Further Explanation|  &nbsp; &nbsp; &nbsp;Sample Comment &nbsp; &nbsp; &nbsp;    |    \n",
    "|-------------------|-------|--------------|-------------------------------------------------------|\n",
    "| Readability                     | 1     | Comments related to readability, style, general project conventions.                                | \"Please split this statement into two separate ones\" |\n",
    "| Naming                          | 2     |                                                                                                     | \"I think foo would be a more appropriate name\" |\n",
    "| Documentation                   | 3     | Comments related to licenses, package info, module documentation, commenting.                       |\"Please add a comment here explaining this logic\" |\n",
    "| Error/Resource Handling         | 4     | Comments related to exception/resource handling, program failure,  termination analysis, resource . |\"Forgot to catch a possible exception here\" |\n",
    "| Control Structures/Program Flow | 5     | Comments related to usage of loops, if-statements, placement of individual lines of code.           |\"This if-statement should be moved after the while loop\" |\n",
    "| Visibility/ Access              | 6     | Comments related to access level for classes, fields, methods and local variables.                  |\"Make this final\" |\n",
    "| Efficiency / Optimization       | 7     |                                                                                                     |\"Many uneccessary calls to foo() here\" |\n",
    "| Code Organization/ Refactoring  | 8     | Comments related to extracting code from methods and classes, moving large chunks of code around.   |\"Please extract this logic into a separate method\" |\n",
    "| Concurrency                     | 9    | Comments related to threads, synchronization, parallelism.                                          |\"This class does not look thread safe\" |\n",
    "| High Level Method Semantics & Design                           | 10    | Comments relating to method design and semantics.                                                           |\"This method should return a String\" |\n",
    "| High Level Class Semantics & Design                           | 11    | Comments relating to class design and semantics.                                                           |\"This should extend Foo\" |\n",
    "| Testing                           | 12    |                                                           |\"is there a test for this?\" |\n",
    "| Other                           | 13    | Comments not relating to categories 1-12.                                                           |\"Looks good\", \"done\", \"thanks\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier Implementation\n",
    "\n",
    "Now we'll discuss our SVM text classifier implementation. This experiment represents a typical supervised learning classification exercise.\n",
    "We'll start by first loading our training data from a local directory which consists of two files representing 2000 manually labelled comment-classification pairs. The [first file](https://raw.githubusercontent.com/Zir0-93/zir0-93.github.io/master/_posts/review_comments%20(1).txt) contains a review comment on each\n",
    "line, while the [second file](https://raw.githubusercontent.com/Zir0-93/zir0-93.github.io/master/_posts/review_comments_labels%20(1).txt)  contains manually determined classifications for each corresponding review comment on each line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/review_comments.txt') as f:\n",
    "    review_comments = f.readlines()\n",
    "    \n",
    "with open('data/review_comments_labels.txt') as g:\n",
    "    classifications = g.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to preprocess the raw data in multiple steps to prepare it for use by our SVM classifier. First, we remove all formatting characters from each comment that are associated with the Markdown syntax. Markdown is a lightweight\n",
    " markup language with plain text formatting syntax. It is designed to be easily converted to HTML and many other formats\n",
    " using a tool by the same name, and more importantly, can be used to write GitHub code review comments. This step is important because the additional formatting related characters introduced by the \n",
    " Markdown standard will negatively impact our classifier's ability to recognize identical words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def formatComment(comment):\n",
    "        comment = re.sub(\"\\*|\\[|\\]|#|\\!|,|\\.|\\\"|;|\\?|\\(|\\)|`.*?`\", \"\", comment)\n",
    "        comment = re.sub(\"\\.|\\(|\\)|<|>\", \" \", comment)\n",
    "        comment = ' '.join(comment.split())\n",
    "        return comment\n",
    "        \n",
    "\n",
    "def formatComments(comments):\n",
    "    for index, comment in enumerate(comments):\n",
    "        comments[index] = formatComment(comment)  \n",
    "\n",
    "formatComments(review_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A note on using stopwords and stemmers.** My experimental results showed that using off the shelf stopword lists and stemmers to preprocess the data decreased the accuracy of the final classifier. This is why I have not used any of these techniques in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step of our preprocessing stage is to convert the comment reviews into numerical feature vectors. This is required to\n",
    "make our review comments amenable for machine learning algorithms. To do this, we will use the bag of words method, which \n",
    "represents a sentence using a feature vector developed based on the number of occurrences of each\n",
    "term, known as *term frequency*. Note that, in this view, the comment `please rename this variable`, is identical\n",
    "to the comment `rename this variable please`. We use the Scikit-learn Python library to create feature vectors for our \n",
    "review comments using the `CountVectorizer` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 4295)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting features from text files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer='word')\n",
    "comments_train_counts = count_vect.fit_transform(review_comments)\n",
    "comments_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wil also experiment with incorporating the inverse document frequency statistic, a common technique used in text classification experiments. To understand how the technique works, consider a commonly occurring term like \"the\". A simple bag of words model based only on term frequency would tend to incorrectly emphasize review comments which happen to use the word \"the\" more frequently, without giving enough weight to the more meaningful terms like \"variable\" and \"naming\". This is problematic as the term \"the\" is not a good keyword to distinguish relevant and non-relevant documents and terms, unlike the less-common words \"variable\" and \"naming\". Hence the inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.\n",
    "\n",
    "**Putting it all together, the weight the td-idf statistic assigns to a given term is:**\n",
    "\n",
    "1. Highest when the term occurs many times within a small number of review comments\n",
    "2. Lower when the term occurs fewer times in a review comment, or occurs in many review comments\n",
    "3. Lowest when the term occurs in virtually all review comments.\n",
    "\n",
    "At this point, we can view each review comment as a vector with one component\n",
    "corresponding to each term in the dictionary, together with a weight for each\n",
    "component that is given by the tf-idf statistic. For dictionary terms that do not occur in\n",
    "a document, this weight is zero. This vector form will prove to be crucial to\n",
    "the scoring and ranking capabilities of our SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 4295)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "comments_train_tfidf = tfidf_transformer.fit_transform(comments_train_counts)\n",
    "comments_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the classifier itself is almost ready, an important consideration now is the amount of training data to use for testing the classifier. After ensuring that atleast 100 review comments for each classification are present in our labeled data set, I experimented with different numbers of review comments\n",
    "to see what gave the best results. 2000 review comments seemed to give a good enough accuracy for our purposes. We will therefore dedicate 80% of our 2000 GitHub review comments data to the training set, which we will use to train our SVM classifier. The remaining 20% of the data will be dedicated to the test set, which we will use to test the performance of the developed classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "comment_train, comment_test, classification_train, classification_test = train_test_split(review_comments, classifications, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can complete our classifier by combining the components developed so far with the scikit SVM classifier using the scikit `Pipeline` module. The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. We also use the scikit `SGDClassifier` module to train our SVM model using Stochastic Gradient Descent (SGD). SGD is an iterative based optimization technique. In this case, the technique modifies the SVM parameters on each training iteration to find a local optimum that produces the best results. We set the number of iterations for our estimator at 1000. As demonstrated below, our developed classifier scored an accuracy of 82% on the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         1\n",
      "       0.84      0.77      0.80        74\n",
      "        10\n",
      "       0.68      0.79      0.73        19\n",
      "        11\n",
      "       1.00      0.74      0.85        23\n",
      "        12\n",
      "       0.88      0.93      0.90        15\n",
      "        13\n",
      "       0.75      0.91      0.82       116\n",
      "         2\n",
      "       0.65      0.88      0.75        17\n",
      "         3\n",
      "       0.91      0.72      0.81        29\n",
      "         4\n",
      "       0.97      0.88      0.92        33\n",
      "         5\n",
      "       0.83      0.80      0.82        25\n",
      "         6\n",
      "       0.82      0.82      0.82        11\n",
      "         7\n",
      "       1.00      0.33      0.50         6\n",
      "         8\n",
      "       1.00      0.52      0.69        23\n",
      "         9\n",
      "       0.73      0.89      0.80         9\n",
      "\n",
      "avg / total       0.83      0.81      0.81       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Support Vector Machines - SVM and calculating its performance\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), \n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='elasticnet',alpha=1e-3, max_iter=1000, random_state=42))])\n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(review_comments, classifications)\n",
    "predicted_svm = text_clf_svm.predict(comment_test)\n",
    "print(classification_report(classification_test, predicted_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An f1-score of 82% did not seem reliable enough for the purpose of classifying over 30 000 GitHub review comments. Moreover, I realized that the incorporation of the tf-idf statistic, while useful in other text classification activities, was not suitable for the classification of review comments. The reason for this can be traced to the treatment of uncommon words in this technique. In a regular document, an uncommon word, like \"abject\" for example, would be valuable in classifying that document. Review comments also contain uncommon terms; however, these terms mostly reference source code entities which we would not want to our classifier to place a major importance on. If our labeled data set for example consisted of a review comment that read, \"The Foo class has some formatting issues.\", we would manually assign the `Readability` classification to this comment. The problem is because `Foo` is an uncommon term, our tf-idf based SVM classifier would highly correlate this term with the `Readability` category, which is undesirable. This is because any future review comments containing the term  `Foo` would be given the `Readability` classification with a very high probability, which is undesired for obvious reasons. A solution to this problem would be to replace any source code entities referenced in review comments by a static string, like `<SYMBOL>` for example; However, this would be difficult to detect accurately. Therefore, we will simply revert the use of the tf-idf statistic, which raises the accuracy of our classifier significantly to 94%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         1\n",
      "       0.91      0.85      0.88        74\n",
      "        10\n",
      "       0.95      0.95      0.95        19\n",
      "        11\n",
      "       1.00      0.91      0.95        23\n",
      "        12\n",
      "       1.00      1.00      1.00        15\n",
      "        13\n",
      "       0.86      0.97      0.91       116\n",
      "         2\n",
      "       1.00      1.00      1.00        17\n",
      "         3\n",
      "       0.93      0.97      0.95        29\n",
      "         4\n",
      "       1.00      0.97      0.98        33\n",
      "         5\n",
      "       1.00      0.88      0.94        25\n",
      "         6\n",
      "       1.00      0.91      0.95        11\n",
      "         7\n",
      "       1.00      0.83      0.91         6\n",
      "         8\n",
      "       1.00      0.87      0.93        23\n",
      "         9\n",
      "       1.00      1.00      1.00         9\n",
      "\n",
      "avg / total       0.94      0.93      0.93       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Support Vector Machines - SVM and calculating its performance\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), \n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='elasticnet',alpha=1e-3, max_iter=1000, random_state=42))])\n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(review_comments, classifications)\n",
    "predicted_svm = text_clf_svm.predict(comment_test)\n",
    "print(classification_report(classification_test, predicted_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying GitHub Review Comments\n",
    "We will now leverage the classifier developed in the previous section to classify over 30000 GitHub review comments from the top 100\n",
    "most forked Java repositories on GitHub. GitHub exposes a REST API that allows developers to interact with the platform, which we will use to mine our Review Comments. In general,\n",
    "an API provides an interface between two systems to interact with each other programmatically. Representational State Transfer (REST) \n",
    "is an architectural style that defines a set of constraints and properties based on HTTP. APIs that conform to the REST architectural \n",
    "style, or RESTful web services, provide interoperability between computer systems on the Internet. We first consume the GitHub REST API to\n",
    "load repository data for the 100 most forked java repositories on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "# URL to consume GitHub REST API to retrieve the top 50 most forked repositories on GitHub\n",
    "url = 'https://api.github.com/search/repositories?q=language:java&sort=forks&order=desc&per_page=100&page=1'\n",
    "# Execute the HTTP GET request\n",
    "resp_text = urllib.request.urlopen(urllib.request.Request(url)).read().decode('UTF-8')\n",
    "# load the JSON response in a python object\n",
    "repos_json_obj = json.loads(resp_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to use the GitHub REST API again to collect a list of all the review comments from each repository. Note, the code below will require you to add your own GitHub OAuth token if you wish to execute it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 26927 review comments.\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "github_access_token = 'GITHUB_ACCESS_TOKEN'\n",
    "###########################################\n",
    "\n",
    "review_comments = []\n",
    "# loop through our list of repositories..\n",
    "for repo in repos_json_obj['items']:\n",
    "    # i is the current page number\n",
    "    for j in range(1, 11):\n",
    "        # URL to consume GitHub REST API to retrieve 100 review comments\n",
    "        url = 'https://api.github.com/repos/' + repo['owner']['login'] \\\n",
    "        + '/' + repo['name'] + '/pulls/comments?direction=desc&per_page=100&page=' + str(j) \\\n",
    "        + '&access_token=' + github_access_token\n",
    "        # Execute the HTTP GET request and store response in object\n",
    "        json_obj = json.loads(urllib\n",
    "                              .request\n",
    "                              .urlopen(urllib.request.Request(url))\n",
    "                              .read()\n",
    "                              .decode('UTF-8'))\n",
    "        # Store all review comments from the response\n",
    "        review_comments.extend(json_obj)\n",
    "    \n",
    "print ('Collected', str(len(review_comments)), 'review comments.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we will categorize each review comment using our SVM classifier, and generate a donut chart demonstrating our results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f72ed0823c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from palettable.tableau import Tableau_10\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Data to plot\n",
    "labels = ['Readability', 'Naming', 'Documentation', 'Error/Resource Handling', \n",
    "'Control Structures/Code Flow', 'Visiblity', 'Efficiency', 'Code Organization/Refactoring',\n",
    "'Concurrency', 'Method Design/Semantics', 'Class Design/Semantics', 'Testing', 'Other']\n",
    "\n",
    "\n",
    "sizes = [0] * 13\n",
    "explode = [0.03] * 13\n",
    "\n",
    "# loop through review comments and score\n",
    "for review_comment in review_comments:\n",
    "    label = int(text_clf_svm.predict([formatComment(review_comment['body'])])[0])\n",
    "    sizes[label - 1] += 1\n",
    "\n",
    "# Create a circle for the center of the plot\n",
    "my_circle=plt.Circle( (0,0), 0.75, color='white')\n",
    "\n",
    "# Plot\n",
    "plt.pie(sizes, labels=labels, colors=Tableau_10.hex_colors,\n",
    "        autopct='%1.0f%%', explode=explode, shadow=True, startangle=140) \n",
    "fig = plt.gcf().set_size_inches(10,10) \n",
    "plt.gca().add_artist(my_circle)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
